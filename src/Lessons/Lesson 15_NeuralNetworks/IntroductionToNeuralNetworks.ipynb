{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Стохастичний градієнтний спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Для прикладу розглянемо задачу оптимізації, яку можна вирішити методом градієнтного спуску: знайти таке значення $\\mu$, при якому наступна функція набуває мінімального значення:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f = \\frac{1}{2} \\sum_{i=1}^{n}{(x_{i} - \\mu)^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&emsp;&emsp;Запишемо похідну та вирішимо задачу звичайним методом градієнтного спуску для деякого набору даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{df}{d \\mu} = -\\sum_{i=1}^{n}{(x_{i} - \\mu)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nadiia.parsons/Documents/python-for-data-science/src/Lessons/Lesson 15_NeuralNetworks/IntroductionToNeuralNetworks.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nadiia.parsons/Documents/python-for-data-science/src/Lessons/Lesson%2015_NeuralNetworks/IntroductionToNeuralNetworks.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadiia.parsons/Documents/python-for-data-science/src/Lessons/Lesson%2015_NeuralNetworks/IntroductionToNeuralNetworks.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadiia.parsons/Documents/python-for-data-science/src/Lessons/Lesson%2015_NeuralNetworks/IntroductionToNeuralNetworks.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39m10_000\u001b[39m)  \u001b[39m# згенерируємо дані\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = np.random.normal(2, 1, size=10_000)  # згенерируємо дані\n",
    "\n",
    "# визначимо цільову функцію задачі\n",
    "def f(x, m):\n",
    "    return 0.5 * np.sum((x - m) ** 2)\n",
    "\n",
    "# визначимо похідну цільової функції\n",
    "def df(x, m):\n",
    "    return -np.sum(x - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mu = 10  # початкова гіпотеза про значення параметра\n",
    "alpha = 0.00001  # параметр швидкості навчання\n",
    "n_iterations = 100  # кількість кроків алгоритму (ітерацій)\n",
    "\n",
    "mu_values = [mu]  # список, до якого будемо записувати проміжні значення\n",
    "for i in range(n_iterations):\n",
    "    mu -= alpha * df(X, mu)  # оновлення параметра\n",
    "    mu_values.append(mu)\n",
    "\n",
    "print(mu)\n",
    "# графік процесу пошуку параметра\n",
    "plt.plot(range(n_iterations + 1), mu_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Запропонований вище метод призводить до правильного рішення. Однак, зверніть увагу, що щоразу при знаходженні похідної нам необхідно підсумовувати всі значення Х. Ми можемо собі це дозволити, якщо набір даних не надто великий і якщо функція похідної не дуже складна, проте, якби у нас були мільярди записів для Х, то ми фізично не змогли цього зробити, оскільки всі значення не помістилися б у пам'яті комп'ютера.\n",
    "\n",
    "&emsp;&emsp;Модифікація методу, яка називається стохастичний градієнтний спуск, полягає в наступному:\n",
    "\n",
    "&emsp;&emsp;1. Розіб'єм дані на невеликі фрагменти, які помістяться в пам'яті (наприклад, по 64 спостереження за раз).\n",
    "\n",
    "&emsp;&emsp;2. Обчислимо значення похідної лише поточного маленького фрагмента даних (такий фрагмент зазвичай називається batch чи mini batch) і оновимо значення параметра користуючись лише цією похідною.\n",
    "\n",
    "&emsp;&emsp;3. Перейдемо до наступного батчу та повторимо процедуру. Одне оновлення називається ітерацією\n",
    "(ітерація чи наближення - у випадку, це один крок, у якому відбувається оновлення параметрів алгоритму). Цикл, у якому ми послідовно перебрали всі дані, які ми маємо, називатимемо епохою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mu = 10  # початкова гіпотеза про значення параметра\n",
    "alpha = 0.00001  # ппараметр швидкості навчання\n",
    "n_epochs = 100  # кількість епох: скільки разів алгоритм \"побачить\" всі дані\n",
    "batch_size = 64  # розмір батча\n",
    "\n",
    "mu_values = [mu]\n",
    "for epoch in range(n_epochs):\n",
    "    i = 0\n",
    "    # пройдемося по всіх даним порційно\n",
    "    while i + batch_size < len(X):\n",
    "        \n",
    "        # сформуємо поточний батч\n",
    "        batch = X[i:i+batch_size]\n",
    "        \n",
    "        # оновимо значення параметра, використовуючи тільки дані батча\n",
    "        mu -= alpha * df(batch, mu)\n",
    "        i += batch_size\n",
    "    \n",
    "    mu_values.append(mu)\n",
    "\n",
    "plt.plot(range(n_epochs + 1), mu_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Як бачимо, ми отримали той самий результат, але шляхом, що вимагає набагато менше пам'яті одночасно. У разі величезних обсягів даних ми таким чином можемо зчитувати їх по 64 спостереження прямо з диска і видаляти їх із пам'яті після обробки.\n",
    "\n",
    "&emsp;&emsp;Стохастичний градієнтний спуск (та його модифікації) - основний алгоритм навчання нейронних мереж.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Базовою одиницею нейронної мережі є перцептрон. Зобразити цей блок графічно можна так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](Single_layer_perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&emsp;&emsp;По-суті, це картинка моделі логістичної регресії. Тут $x_{i}$ - назва змінної, $w_{i}$ - вага моделі, на яку домножується кожна змінна.\n",
    "Червоний круг відповідає підсумовуванню, тобто це скалярний добуток $w^{T}x$, а $f(x)$ - логістична (чи якась інша) функція.\n",
    "\n",
    "&emsp;&emsp;Вирішимо задачу класифікації ірисів, використовуючи логістичну регресію у вигляді запропонованого вище персептрону"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# вивантажимо дані ірисів\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# візьмемо лише 2 класи і розіб'ємо на навчальну та тестову групи\n",
    "X, y = X[y >= 1], y[y >= 1]\n",
    "y = np.where(y == 2, 1, 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Тепер ініціалізуємо модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# фіксація випадкового стану\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()  # ініціалізація порожньої моделі\n",
    "\n",
    "# додамо один перцептрон у модель\n",
    "model.add(Dense(\n",
    "    units=1,  # кількість перцептронів\n",
    "    activation='sigmoid'  # функція активації: перетворення на виході моделі\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;На наступному кроці необхідно визначитися з алгоритмом навчання та скомпілювати модель. Як було сказано вище, основним алгоритмом навчання нейронних мереж є стохастичний градієнтний спуск, однак у нього є кілька варіацій з додатковими параметрами. Скористаємося найпростішим варіантом, імплементованим у keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),  # алгоритм оптимізації та швидкість навчання\n",
    "    loss='binary_crossentropy',  # функція втрат (та сама, що в логістичної регресії)\n",
    "    metrics=['accuracy']  # додаткова метрика якості моделі\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Тепер можемо приступати до навчання"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=X_train, y=y_train,  # навчальні дані\n",
    "    batch_size=8,  # розмір батча\n",
    "    epochs=300,  # кількість епох навчання\n",
    "    validation_data=(X_test, y_test)  # дані для валідації\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Визначимо функцію, яка будує графік зміни помилки на навчальній та тестовій групі залежно від епохи та подивимося, як проходив процес навчання"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_history(hist):\n",
    "    train_loss, test_loss = hist.history['loss'], hist.history['val_loss']\n",
    "    x = np.arange(len(train_loss))\n",
    "    plt.plot(x, train_loss, label='Train loss')\n",
    "    plt.plot(x, test_loss, label='Test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&emsp;&emsp;На відміну від логістичної регресії, навчання нейронної мережі не зупиниться самостійно. Його прийнято переривати на тому моменті, коли спостерігається погіршення точності на тестовому наборі (так само, як і для градієнтного бустингу)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Розглянемо більш цікаву задачу, на яку логістична регресія неспроможна дати задовільного рішення: завдання виключаючого або (XOR). Для цього згереруємо двовимірні дані, кожен вимір яких може набувати від'ємних і додатніх значень. Якщо обидва виміри додатні чи обидва від'ємні, то точка належить одному класу, інакше - іншому."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.randn(300, 2)  # згенеруємо дані\n",
    "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), dtype=int)  # створимо класи\n",
    "\n",
    "# відобразимо отримані класи на графіку\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Спробуємо застосувати до даних ту саму нейронну мережу, яка імітує модель логістичної регресії:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# фіксація випадкового стану\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()  # ініціалізація порожньої моделі\n",
    "\n",
    "# додамо один персептрон у модель\n",
    "model.add(Dense(\n",
    "    units=1,  # кількість перцептронів\n",
    "    activation='sigmoid'  # функція активації: перетворення на виході моделі\n",
    "))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),  # алгоритм оптимізації та швидкість навчання\n",
    "    loss='binary_crossentropy',  # функція втрат (та сама, що в логістичної регресії)\n",
    "    metrics=['accuracy']  # додаткова метрика якості моделі\n",
    ")\n",
    "\n",
    "history = model.fit(x=X, y=y, batch_size=8, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Як бачимо, точність залишається близько 50% скільки б модель не навчалася. Це пов'язано з тим, що межа прийняття рішень в логістичної регресії - пряма лінія, а наші дані неможливо розділити однією прямою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(X, y, clf=model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Щоб впоратися з таким завданням необхідно вивчити більш складну функцію прийняття рішень. Цього можна досягти, якщо додати прихований шар у нашу модель:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;На картинці вище кожен синій круг - це змінна, яка надходить на вхід моделі. Кожен фіолетовий круг (нейрон мережі) відповідає операції скалярного добутку свого вектора $w_{i}$ на набір вхідних змінних $x$. Таким чином, на виході прихованого шару ми отримаємо 3 нові змінні:\n",
    "\n",
    "$$y_{1} = w_{1}^{T}x$$\n",
    "$$y_{2} = w_{2}^{T}x$$\n",
    "$$y_{3} = w_{3}^{T}x$$\n",
    "\n",
    "&emsp;&emsp;Ми можемо об'єднати всі вектори $w$ в одну матрицю $W$ і записати в більш компактній формі:\n",
    "\n",
    "$$y = Wx$$\n",
    "\n",
    "&emsp;&emsp;В останньому нейроні (помаранчевий круг) буде навчений ще один вектор $z$, при домноженні на який ми отримаємо фінальний результат:\n",
    "\n",
    "$$output = z^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Однак зверніть увагу, що $output = z^{T}y = z^{T}Wx = Bx$, де $B = z^{T}W$. Виходить, що не було сенсу вивчати матрицю параметрів $W$ і окремо вектор $z$, якщо можна було відразу вивчити вектор $B$, що призвело б нас до початкової форми моделі. Щоб додати ще один шар мережі був сенс до його виходу (вектор $y$) необхідно застосувати якесь нелінійне перетворення, яке не дозволить рівність $B = z^{T}W$. Таке перетворення називається функцією активації нейрона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Найпопулярнішими функціями активації є сигмоїд та ReLU (rectified linear unit), які мають такий вигляд:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(121)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('Sigmoid function')\n",
    "plt.subplot(122)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title('ReLU function')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Ускладнимо архітектуру нашої моделі, додавши додатковий прихований шар нейронів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# фіксація випадкового стану\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()  # ініціалізація порожньої моделі\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.03),  # алгоритм оптимізації та швидкість навчання\n",
    "    loss='binary_crossentropy',  # функція втрат (та сама, що в логістичної регресії)\n",
    "    metrics=['accuracy']  # додаткова метрика якості моделі\n",
    ")\n",
    "\n",
    "history = model.fit(x=X, y=y, batch_size=8, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, clf=model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;У загальному випадку нейронна мережа може мати довільну кількість як прихованих шарів, так і нейронів у кожному шарі мережі. Для завдання класифікації за наявності кількох класів нам знадобиться кілька виходів моделі - по одному нейрону на кожен клас."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](neural_net.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Для прикладу побудуємо модель класифікації ірисів, використовуючи всі 3 класи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "y = pd.get_dummies(y).values  # цільову змінну представимо у вигляді one-hot вектора\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "mdl = Sequential()\n",
    "mdl.add(Dense(units=3, activation='relu'))\n",
    "\n",
    "# На виході моделі ми хочемо отримати вектор, що є розподілом ймовірностей для 3х класів.\n",
    "# Це означає, що кількість нейронів має бути 3 і нам необхідно підібрати таку функцію активації, щоб\n",
    "# розподіл вважалося допустимим (всі елементи були невід'ємними та у сумі давали 1). Хорошим кандидатом\n",
    "#у такому разі є функція softmax\n",
    "mdl.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Оскільки у нас тепер 3 класи, нам буде потрібна інша функція втрат. Зазвичай для класифікаційних завдань добре підходить категоріальна крос-ентропія як розширення логістичної функції втрат. Функція має таку формулу:\n",
    "\n",
    "$$Loss = -\\sum_{i=1}^{n}{\\sum_{j=1}^{k}{y_{ij}log(p_{ij})})}$$\n",
    "\n",
    "&emsp;&emsp;Тут $k$ - кількість класів. Для двох класів формула спрощується до вже відомої функції логістичної втрати."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mdl.compile(\n",
    "    optimizer=SGD(learning_rate=0.005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = mdl.fit(\n",
    "    x=X_train, \n",
    "    y=y_train,\n",
    "    batch_size=8,\n",
    "    epochs=200,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Основи работи із зображеннями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Згортковою нейронною мережею називається така, яка включає операцію згортки. Такого типу мережі - це основний алгоритм для роботи із зображеннями та багатьма завданнями обробки сигналів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&emsp;&emsp;Почнемо з того, що розберемося, що собою являє картинка. По-суті, в очах комп'ютера, картинка - це тривимірний масив, який у перших двох осях містить координати пікселя на зображенні, а в осі, що залишилася, його яскравість. Якщо картинка кольорова, то яскравість задається трьома числами: яскравістю червоного, зеленого та блакитного кольорів. Якщо картинка чорно-біла, то достатньо одного числа.\n",
    "\n",
    "&emsp;&emsp;Яскравість зазвичай знаходиться в інтервалі від 0 до 255, де 0 відповідає повній відсутності кольору (у чорно-білому випадку це чорний піксель), а 255 - максимальної яскравості (ідеально білого кольору). Всі проміжні значення є відтінками сірого кольору."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Наприклад, ми можемо намалювати кольорову лінію на чорному фоні, використовуючи numpy таким чином:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# створимо тривимірний масив із нулів, що відповідає чорному фону\n",
    "img = np.zeros((28, 28, 3), dtype=np.int32)\n",
    "img[:, 14, :] = [124, 50, 255]  # додамо лінію довільного кольору як стовпець\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;У більшості випадків при роботі з зображеннями об'єм даних занадто великий, щоб поміститися в оперативну пам'ять комп'ютера. Тому прийнято зчитувати картинки порційно (батч) прямо з диска. При відповідній структурі папок з картинками ми можемо скористатися вбудованим класом keras, який дозволяє зчитувати картинки та передавати їх моделі для навчання.\n",
    "\n",
    "&emsp;&emsp;Для завдання класифікації структура папки має бути така:\n",
    "\n",
    "&emsp;&emsp;1. Навчальні та тестові дані повинні знаходитись у різних папках (наприклад, train/test)\n",
    "\n",
    "&emsp;&emsp;2. У кожній папці має бути стільки папок, скільки є класів у завданні. Назва папки повинна відповідати порядковому номеру класу в дослідженні і в цій папці повинні знаходитись лише картинки, що належать цьому класу.\n",
    "\n",
    "&emsp;&emsp;Розглянемо приклад із розпізнаванням рукописних цифр. По суті, це завдання класифікації: на вхід подається картинка з якоюсь цифрою, на виході ми хочемо отримати число від 0 од 9, яке відповідає написаній на малюнку цифрі. Розбивши дані на 2 папки (testing/training) у кожній створимо по 10 папок із назвами від 0 до 9, як це зроблено в папці з даними MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Ініціалізуємо генератори картинок для навчального та тестового набору. При роботі із зображеннями прийнято\n",
    "# нормувати яскравість так, щоб вона була в інтервалі від 0 до 1, тому передамо параметр rescale 1/255,\n",
    "# що відповідає поділу на 255 кожного пікселя оригінальної картинки.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# створення генератора картинок для навчального набору\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory='./Data/MNIST/training',  # папка, звідки читати картинки\n",
    "    target_size=(28, 28),  # розмір картинки (якщо оригінальне зображення не відповідає, то розмір буде змінено)\n",
    "    color_mode=\"grayscale\",  # колірна гама (у нашому випадку картинки чорно-білі)\n",
    "    batch_size=8,  # розмір батча (скільки картинок зчитувати за один раз)\n",
    "    class_mode=\"categorical\",  # тип завдання (класи будуть взяті з назви папок, у яких лежать картинки)\n",
    "    shuffle=True,  # чи перемішувати картинки при зчитуванні або читати по порядку\n",
    "    seed=42  # фіксація випадкового стану\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Проходячи циклом по створеному нами генератору ми будемо на кожному кроці отримувати по 8 картинок і по 8 класів, які з ними асоціюються у форматі one-hot вектора. Такий цикл може тривати нескінченно, тому виходити з нього завжди потрібно вручну."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for img, label in train_gen:    \n",
    "    # цикл, в якому додамо картинку на графік\n",
    "    for i in range(img.shape[0]):        \n",
    "        image_label = np.argmax(label[i])  # отримаємо клас картинки з one-hot вектора\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        # для чорно-білої картинки третя вісь є зайвою, тому позбудемося її і оголосимо гаму як \"gray\"\n",
    "        plt.imshow(img[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f'This is {image_label}')    \n",
    "    print(label)\n",
    "    break\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Збагачення даних (аугментація)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;У широкому значенні слова аугментація - це будь-який спосіб додати спостереження або змінні до даних. Наприклад, якщо завдання кредитного скорингу є спосіб отримати доступ до даних бюро кредитних історій і додати цю інформацію в модель, це аугментація.\n",
    "\n",
    "&emsp;&emsp;При роботі із зображеннями або сигналами під аугментацією зазвичай розуміють процес створення нових спостережень шляхом зміни існуючих. Справа в тому, що для людського ока яскравість картинки, положення об'єкта і ще багато деталей зображення часто не є критичними при розпізнаванні класу, однак через те, як машини бачать зображення, для комп'ютера це може бути далеко не настільки очевидно.\n",
    "\n",
    "&emsp;&emsp;Наприклад, стоїть завдання знайти картинки, на яких присутня тільки вертикальна лінія:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "for i in range(8):    \n",
    "    img = np.zeros((28, 28, 3), dtype=np.int32)  # ініціалізація чорної картинки\n",
    "    img += np.random.randint(0, 100, size=1)[0]  # довільна зміна яскравості фону\n",
    "    line_position = np.random.randint(0, 28, size=1)[0]  # довільний вибір положення лінії\n",
    "    line_color = np.random.randint(0, 255, 3)  # довільний колір лінії\n",
    "    \n",
    "    img[:, line_position, :] = line_color  # додавання лінії на фон\n",
    "    \n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Людина розуміє, що всі картинки вище належать до одного класу: картинки, на яких є вертикальна лінія. Однак для машини це всі різні картинки. Такі перетворення допомагають донести інформацію про несуттєві атрибути зображення. У цьому випадку несуттєвим є положення лінії, колір фону та самої лінії.\n",
    "\n",
    "&emsp;&emsp;Можемо перевизначити наш генератор з найбільш популярними типами аугментації:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=5, # випадкові поворот картинки в інтервалі + - 5 градусів\n",
    "    width_shift_range=2, # випадковий зсув картинки по горизонталі на +- 2 пікселя\n",
    "    height_shift_range=2, # випадковий зсув картинки по вертикалі на +- 2 пікселя\n",
    "    brightness_range=(0.90, 1.1), # інтервал випадкової зміни яскравості (+- 10%)\n",
    "    zoom_range=(0.95, 1.05), # інтервал випадкового наближення/віддалення (+- 5%)\n",
    "    horizontal_flip=False, # чи робити випадково дзеркальне відображення картинки по горизонталі\n",
    "    vertical_flip=False, # чи робити випадково дзеркальне відображення картинки по вертикалі\n",
    "    preprocessing_function=None, # довільна функція, яка застосовує перетворення користувача\n",
    "    rescale=1./255 # приведення яскравості до інтервалу [0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory='./Data/MNIST/training', # папка, звідки читати картинки\n",
    "    target_size=(28, 28), # розмір картинки (якщо оригінальна картинка не відповідає розміру, то розмір буде змінено)\n",
    "    color_mode=\"grayscale\", # кольорова гама (у нашому випадку картинки чорно-білі)\n",
    "    batch_size=8, # розмір батча (скільки картинок зчитувати за один раз)\n",
    "    class_mode=\"categorical\", # тип завдання (класи будуть взяті з назви папок, в яких лежать картинки)\n",
    "    shuffle=True, # чи перемішувати картинки при зчитуванні або читати по порядку\n",
    "    seed=42 # фіксація випадкового стану\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;З огляду на те, що кожна картинка з додаванням випадкових модифікацій, стає практично унікальною, а генерувати такі картинки ми можемо нескінченно, то, по суті, ми отримуємо дані нескінченного розміру. Однак на практиці додаткової інформації з ходом навчання стає все менше і модель зупиняється на якийсь епосі після погіршення помилки на тесті.\n",
    "\n",
    "&emsp;&emsp;При аугментації дуже важливо розуміти, які зміни дійсно є некритичними і дозволять збагатити дані, а які ні. Наприклад, якщо завдання варто зрозуміти тип хмари, то картинку можна перевернути ногами вгору і від цього хмара залишиться хмарою. Але якщо так само зробити з цифрами, то 6 перетвориться на 9, що призведе до небажаної зміни класу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Створимо генератор тестових зображень так само:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_gen = test_datagen.flow_from_directory(\n",
    "    directory='./Data/MNIST/testing', # папка, звідки читати картинки\n",
    "    target_size=(28, 28), # розмір картинки (якщо оригінальна картинка не відповідає по розміру, то розмір буде змінено)\n",
    "    color_mode=\"grayscale\", # кольорова гама (у нашому випадку картинки чорно-білі)\n",
    "    batch_size=8, # розмір батча (скільки картинок зчитувати за один раз)\n",
    "    class_mode=\"categorical\", # тип завдання (класи будуть взяті з назви папок, в яких лежать картинки)\n",
    "    shuffle=True, # чи перемішувати картинки при зчитуванні або читати по порядку\n",
    "    seed=42 # фіксація випадкового стану\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Операція згортки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Картинка - це приклад неструктурованих даних, тобто таких, у яких змінні не знаходяться в одній зручній таблиці. Для того, щоб якийсь алгоритм міг працювати з такими даними (наприклад, вирішувати задачу класифікації зображень), нам потрібно якимось чином привести дані з неструктурованого формату до структурованого.\n",
    "\n",
    "&emsp;&emsp;Повернемося до прикладу розпізнавання картинки, де зображена пряма лінія. Ми розуміємо шаблон, за яким картинку з вертикальною лінією можна відрізнити від картинки з горизонтальною. Передати наші знання машині можна шляхом визначення маски, скажімо, наступного формату:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vert_mask = -np.ones((3, 3), dtype=np.int32)\n",
    "vert_mask[:, 1]  = 2\n",
    "print(vert_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Тобто, якщо десь на картинці буде ділянка, схожа на маску в тому сенсі, що з трьох рядів пікселів тільки центральний заповнений якимось кольором, то швидше за все ця картинка містить вертикальну лінію.\n",
    "\n",
    "&emsp;&emsp;Застосовується маска таким чином:\n",
    "\n",
    "&emsp;&emsp;1. Накладається на якусь ділянку зображення\n",
    "\n",
    "&emsp;&emsp;2. Усі елементи маски перемножуються зі значеннями яскравості відповідних пікселів і підсумовуються (така операція називається згорткою)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# створимо картинку з вертикальною лінією\n",
    "img = np.zeros((4, 4), dtype=np.float64)\n",
    "img[:, 2] = 1\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Оскільки ми не знаємо, куди потрібно прикласти маску, спробуємо на всі можливі позиції на картинці зі змішенням в 1 піксель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def convolve(arr1, arr2):\n",
    "    \"Визначимо операцію згортки для двох масивів\"\n",
    "    return np.sum(arr1 * arr2)\n",
    "\n",
    "def apply_mask(image, mask):\n",
    "    \"Визначимо функцію, яка застосовує маску до картинки на всіх можливих позиціях\"\n",
    "    \n",
    "    # визначимо скільки є можливих положень маски по горизонталі та вертикалі\n",
    "    x_positions = image.shape[0] - mask.shape[0] + 1\n",
    "    y_positions = image.shape[1] - mask.shape[1] + 1\n",
    "    \n",
    "    # ініціалізуємо масив, в якому зберігатимемо результати згортки\n",
    "    res = np.empty((x_positions, y_positions))\n",
    "    \n",
    "    # застосуємо маску до всіх можливих положень картинки та збережемо результати\n",
    "    for i in range(x_positions):\n",
    "        for j in range(y_positions):\n",
    "            res[i, j] = convolve(mask, image[i:i+mask.shape[0], j:j+mask.shape[1]])\n",
    "    \n",
    "    return res\n",
    "\n",
    "print(apply_mask(img, vert_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Як і очікувалося, там де маска \"знайшла\" свій шаблон, значення згортки дорівнює 6, а там, де не \"знайшла\", то значення рівне -3. Визначимо ще маску, яка шукатиме горизонтальну лінію і спробуємо вирішити задачу розрізнення картинок з горизонтальною і вертикальною."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "horizontal_mask = -np.ones((3, 3), dtype=np.int32)\n",
    "horizontal_mask[1, :] = 2\n",
    "\n",
    "print(horizontal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(apply_mask(img, horizontal_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Тепер згенеруємо 2 набори даних (картинок з різними лініями різного відтінку сірого кольору):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_images = 100\n",
    "\n",
    "data, labels = [], []\n",
    "for i in range(num_images):\n",
    "    img = np.zeros((4, 4), dtype=np.int32) # створимо чорний фон\n",
    "    is_vertical = np.random.randint(0, 2, size=1)[0] # виберемо тип лінії випадково\n",
    "    line_position = np.random.randint(1, 3, size=1)[0] # виберемо випадково положення лінії\n",
    "    brightness = np.random.randint(30, 255, size=4) # виберемо яскравість лінії\n",
    "    \n",
    "    # заповнимо лінію на зображенні\n",
    "    if is_vertical == 1:\n",
    "        img[:, line_position] = brightness\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        img[line_position, :] = brightness\n",
    "        labels.append(0)\n",
    "        \n",
    "    data.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(data[i], cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Тепер розглянемо результат застосування обох наших масок до першої картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "horizontal_res = apply_mask(data[0], horizontal_mask)\n",
    "vertical_res = apply_mask(data[0], vert_mask)\n",
    "\n",
    "plt.imshow(data[0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('Горизонтальна маска:')\n",
    "print(horizontal_res)\n",
    "print('Вертикальна маска:')\n",
    "print(vertical_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;З результатів видно, що шаблон горизонтальної маски виконується частково, тому отримані для неї результати згортки менші, ніж для вертикальної. Однак наша початкова проблема все ще не вирішена: нам потрібно було отримати таблицю змінних з картинки, а ми отримали іншу картинку просто меншого розміру.\n",
    "\n",
    "&emsp;&emsp;Перевести результати застосування маски в змінні табличного виду можна двома способами:\n",
    "\n",
    "&emsp;&emsp;1. Розглядати усі результати застосування маски як змінні. Тобто ми просто зробимо отримані масиви \"плоськими\", після чого продовжимо працювати з 8 змінними.\n",
    "\n",
    "&emsp;&emsp;2. Застосувати операцію **пулінгу**. Ця операція полягає у агрегації результатів застосування маски. Наприклад, у нашому випадку ми могли б взяти максимальний результат згортки з чотирьох або середній.\n",
    "\n",
    "&emsp;&emsp;Нарешті, створимо наші змінні як максимальний результат згортки для кожної маски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "for img in data:\n",
    "    features.append([\n",
    "        np.max(apply_mask(img, horizontal_mask)),\n",
    "        np.max(apply_mask(img, vert_mask))\n",
    "    ])\n",
    "    \n",
    "features = pd.DataFrame(features, columns=['horizontal', 'vertical'])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Тепер ми можемо скористатися будь-яким алгоритмом класифікації, щоб легко відрізнити одні картинки від інших:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(features['horizontal'], features['vertical'], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Згорткові нейронні мережі"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Проблема в тому, що на реальних завдачах створити досить хороші маски зазвичай немає можливості. Але виявляється, що їх можна вивчити із самих даних. Задамо одну маску як таблицю параметрів:\n",
    "\n",
    "$$mask = \\begin{pmatrix} w_{1} w_{2} w_{3} \\\\ w_{4} w_{5} w_{6} \\\\ w_{7} w_{8} w_{9} \\end{pmatrix}$$\n",
    "\n",
    "&emsp;&emsp;Початкові значення цих параметрів можна згенерувати випадково. Застосовуючи таку маску до зображення і знаючи помилку моделі, ми можемо отримати похідну функції втрат кожного елемента маски, який тепер представлений параметром. При цьому ми можемо ініціалізувати довільну кількість масок, розширюючи тим самим множину шаблонів, які ми шукатимемо на зображенні."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;На практиці згорткові нейронні мережі виглядають як послідовне застосування набору масок (заданих у вигляді параметрів, що навчаються), активацій (як правило ReLU) і операцій пулінгу. Тобто ми можемо визначити маски, які застосовуватимуться до результатів застосування інших масок. Так, у прикладі вище ми після застосування маски отримали з картинки розміру 4х4 картинку розміру 2х2, що дозволяє ще раз застосувати до такої маски відповідного розміру.\n",
    "\n",
    "&emsp;&emsp;У самому кінці моделі всі параметри зазвичай призводять до вигляду одновимірного масиву і нейронна мережа перетворюється на багатошаровий перцептрон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# визначимо архітектуру мережі для класифікації рукописних цифр\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()  # ініціалізація моделі\n",
    "# додамо перший шар, який складатиметься із застосування масок, що навчаються\n",
    "model.add(Conv2D(\n",
    "    filters=8,  # кількість різних масок, які ми хочемо навчити\n",
    "    kernel_size=4,  # розмір однієї маски (4х4 пікселя)\n",
    "    activation='relu',  # активація після застосування маски\n",
    "    input_shape=(28, 28, 1)  # розмір картинки, яка буде подана на вхід\n",
    "))\n",
    "# додамо операцію пулінга, при цьому вкажемо розмір вікна: брати максимум з розмірів розміру 2х2\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# додамо ще один шар масок та пулінг\n",
    "model.add(Conv2D(filters=16, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "# наведемо мережу до одномірного формату і додамо шар нейронів, що навчається\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))  # останній шар з активацією для завдання класифікації"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Тепер ми можемо переглянути структуру нашої мережі:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "&emsp;&emsp;Подальший процес навчання складається з тих самих кроків. Для початку скомпілюємо модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "&emsp;&emsp;Навчання моделі з використанням генераторів даних виглядає так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=train_gen,\n",
    "    validation_data=test_gen,\n",
    "    epochs=3,\n",
    "    workers=8,\n",
    "    steps_per_epoch=len(train_gen),\n",
    "    validation_steps=len(test_gen)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist_fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми будемо використовувати набір даних [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist), який містить 70 000 зображень у відтінках сірого в 10 категоріях. На зображеннях показано окремі предмети одягу з низькою роздільною здатністю (28 на 28 пікселів), як показано тут:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST призначений як додаткова заміна класичного набору даних [MNIST](http://yann.lecun.com/exdb/mnist/), який часто використовується як «Hello, World» програм машинного навчання для комп’ютерного зору. Набір даних MNIST містить зображення рукописних цифр (0, 1, 2 тощо) у форматі, ідентичному формату предметів одягу, які ви тут використовуватимете.\n",
    "\n",
    "Ми використовуємо Fashion MNIST для різноманітності, а також тому, що це дещо складніша проблема, ніж звичайний MNIST. Обидва набори даних є відносно невеликими та використовуються для перевірки того, що алгоритм працює належним чином. Вони є хорошою відправною точкою для тестування та налагодження коду.\n",
    "\n",
    "Тут 60 000 зображень використовуються для навчання мережі та 10 000 зображень для оцінки того, наскільки точно мережа навчилася класифікувати зображення. Ви можете отримати доступ до Fashion MNIST безпосередньо з TensorFlow. Імпортуйємо та завантажуємо дані Fashion MNIST безпосередньо з TensorFlow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "#fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантаження набору даних повертає чотири масиви NumPy:\n",
    "\n",
    "* Масиви `train_images` і `train_labels` є *навчальним набором* — даними, які модель використовує для навчання.\n",
    "* Модель перевіряється на основі *тестового набору*, масивів `test_images` і `test_labels`.\n",
    "\n",
    "Зображення являють собою масиви 28x28 NumPy із значеннями пікселів у діапазоні від 0 до 255. *Мітки* — це масив цілих чисел у діапазоні від 0 до 9. Вони відповідають *класу* одягу, який представляє зображення:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Кожне зображення зіставляється з однією міткою. Оскільки *назви класів* не входять до набору даних, збережмо їх тут, щоб використовувати пізніше під час побудови зображень:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналіз даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте вивчимо формат набору даних перед навчанням моделі. Нижче показано, що в навчальному наборі є 60 000 зображень, кожне зображення представлене як 28 x 28 пікселів:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попередня обробка даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед навчанням мережі дані необхідно попередньо обробити. Якщо ми перевіримо перше зображення в навчальному наборі, ми побачемо, що значення пікселів знаходяться в діапазоні від 0 до 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Змінемо ці значення в діапазоні від 0 до 1, перш ніж передати їх у модель нейронної мережі. Для цього розділемо значення на 255. Важливо, щоб *навчальний набір* і *тестовий набір* були попередньо оброблені однаково:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб переконатися, що дані мають правильний формат і що ми готові створювати та навчати мережу, давайте відобразимо перші 25 зображень із навчального набору та відобразимо назву класу під кожним зображенням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Побудуємо модель\n",
    "\n",
    "Побудова нейронної мережі вимагає налаштування рівнів моделі, а потім компіляції моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Налаштування шарів\n",
    "\n",
    "Основним будівельним блоком нейронної мережі є *шар*. Шари витягують представлення з даних, що вводяться в них. \n",
    "\n",
    "Більшість нейронних мереж складається з об’єднання простих шарів. Більшість шарів, наприклад `tf.keras.layers.Dense`, мають параметри, які вивчаються під час навчання."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      3\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mFlatten(input_shape\u001b[39m=\u001b[39m(\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m ])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перший** шар у цій мережі, `tf.keras.layers.Flatten`, перетворює формат зображень із двовимірного масиву (28 на 28 пікселів) на одновимірний масив (28 *). 28 = 784 пікселів). Цей шар не має параметрів для вивчення, він лише переформатує дані.\n",
    "\n",
    "Після вирівнювання пікселів мережа складається з послідовності двох шарів `tf.keras.layers.Dense`. Це щільно з’єднані або повністю з’єднані нейронні шари. Перший «щільний» шар має 128 вузлів (або нейронів). Другий (і останній) шар повертає масив довжиною 10. Кожен вузол містить оцінку, яка вказує, що поточне зображення належить до одного з 10 класів.\n",
    "\n",
    "### Скомпілюйте модель\n",
    "\n",
    "Перед тим, як модель буде готова до навчання, їй потрібно ще кілька налаштувань. Вони додаються на етапі *компіляції* моделі:\n",
    "\n",
    "* *Функція втрат* — вимірює, наскільки точна модель під час навчання. Ми хочемо мінімізувати цю функцію, щоб «спрямувати» модель у правильному напрямку.\n",
    "* *Оптимізатор* — таким чином модель оновлюється на основі даних, які вона бачить, і її функції втрат.\n",
    "* *Метриики* — використовується для моніторингу етапів навчання та тестування. У наступному прикладі використовується *точність*, частка правильно класифікованих зображень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Навчання моделі\n",
    "\n",
    "Навчання моделі нейронної мережі вимагає наступних кроків:\n",
    "\n",
    "1. Подайти навчальні дані в модель. У цьому прикладі навчальні дані містяться в масивах train_images і train_labels.\n",
    "2. Модель вчиться асоціювати зображення та мітки.\n",
    "3. Ми просимо модель зробити прогнози щодо тестового набору — у цьому прикладі масиву test_images.\n",
    "4. Переконатися, що передбачення збігаються з мітками з масиву test_labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб розпочати навчання, викличемо метод `model.fit` — так він називається, тому що він «підлаштовує» модель під навчальні дані:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Оцінка точность\n",
    "\n",
    "Далі порівняємо, як модель працює на тестовому наборі даних:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виявляється, точність тестового набору даних трохи менша, ніж точність навчального набору даних. Цей розрив між точністю навчання та точністю тесту означає *перенавчання*. Перенавчання відбувається, коли модель машинного навчання працює гірше на нових, раніше невидимих вхідних даних, ніж на навчальних даних. Перенавчена модель «запам’ятовує» шум і деталі в навчальному наборі даних до точки, коли це негативно впливає на ефективність моделі на нових даних."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормалізація ваг\n",
    "\n",
    "«Проста модель» у цьому контексті — це модель, у якій розподіл значень параметрів має меншу ентропію (або модель із зовсім меншою кількістю параметрів, як ми бачили  вище). Таким чином, звичайний спосіб пом’якшити перенавчання полягає в тому, щоб накласти обмеження на складність мережі, змушуючи її ваги приймати лише малі значення, що робить розподіл значень ваг більш «регулярним». Це називається «регуляризацією ваги», і це робиться шляхом додавання до функції втрат мережі вартості, пов’язаної із великими вагами. Ця вартість має два варіанти:\n",
    "\n",
    "* Регулярізація L1, де додана вартість пропорційна абсолютному значенню вагових коефіцієнтів (тобто тому, що називається «нормою L1» ваг).\n",
    "* Регулярізація L2, де додана вартість пропорційна квадрату значення вагових коефіцієнтів (тобто тому, що називається квадратом «норми L2» ваг). Регуляризація L2 також називається спадом ваги в контексті нейронних мереж. \n",
    "\n",
    "Регулярізація L1 підштовхує ваги до точного нуля, заохочуючи розріджену модель. Регулярізація рівня L2 штрафуватиме параметри ваг, не роблячи їх розрідженими, оскільки для малих ваг штраф буде нульовим. одна з причин, чому L2 є більш поширеною.\n",
    "\n",
    "У tf.keras регулярізація ваги додається шляхом передачі екземплярів регуляризатора ваги до шарів як аргументів. Давайте зараз додамо регулярізацію ваги L2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Застосування L2 регуляризації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m regularizers\n\u001b[1;32m      2\u001b[0m model_l2 \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      3\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mFlatten(input_shape\u001b[39m=\u001b[39m(\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, kernel_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39ml2(\u001b[39m0.001\u001b[39m)),\n\u001b[1;32m      5\u001b[0m     keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m ])\n\u001b[1;32m      7\u001b[0m model_l2\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      9\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model_l2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "model_l2.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_l2.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_l2, test_acc_l2 = model_l2.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як можна побачити вище, переонавчання певною мірою усунуто з моделі, але за рахунок продуктивності."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Додавання вилученя\n",
    "Dropout є одним із найефективніших і найчастіше використовуваних методів регуляризації для нейронних мереж, розроблених Хінтоном та його студентами з Університету Торонто.\n",
    "\n",
    "Інтуїтивно зрозуміле пояснення вилучення полягає в тому, що оскільки окремі вузли в мережі не можуть покладатися на вихідні дані інших, кожен вузол повинен виводити функції, які є корисними самостійно.\n",
    "\n",
    "Вилучення, застосоване до шару, складається з випадкового «випадання» (тобто встановленого на нуль) ряду вихідних характеристик шару під час навчання. Скажімо, даний рівень зазвичай повертає вектор [0,2, 0,5, 1,3, 0,8, 1,1] для даного вхідного зразка під час навчання; після застосування вилучення цей вектор матиме кілька нульових записів, розподілених випадковим чином, наприклад. [0, 0,5, 1,3, 0, 1,1].\n",
    "\n",
    "У tf.keras ви можете запровадити вилучення в мережі через рівень Dropout, який застосовується до виводу шару безпосередньо перед цим.\n",
    "\n",
    "Давайте додамо два шари Dropout у нашу мережу, щоб побачити, наскільки добре вони справляються зі зменшенням перенавчання:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "model_dropout = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "model_dropout.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_dropout.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_dropout, test_acc_dropout = model_dropout.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми застосували відсіювання на одному шарі з 128 вузлів, зробивши вихід 30% вузлів нулями, ми змогли зменшити перенавчання більшою мірою, ніж регулярізація l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Комбінація L2 + вилучення"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model_l2_dropout = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "model_l2_dropout.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_l2_dropout.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_l2_dropout, test_acc_l2_dropout = model_l2_dropout.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc_l2_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Робимо прогноз\n",
    "\n",
    "Навчену модель ми можемо використовувати, щоб робити прогнози щодо деяких зображень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential(\n",
    "    [model, tf.keras.layers.Softmax()]\n",
    ")\n",
    "predictions = probability_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут модель передбачила мітку для кожного зображення в наборі для тестування. Давайте подивимося на перший прогноз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Побудуємо це на графіку, щоб переглянути повний набір прогнозів із 10 класів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "    else:\n",
    "    color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевіка прогнозу\n",
    "Наввчену модель ми можемо використовувати, щоб робити прогнози щодо деяких зображень."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте подивимося на 0-е зображення, прогнози та масив прогнозів. Мітки правильного прогнозу сині, а неправильні – червоного. Число дає відсоток (зі 100) для прогнозованої мітки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i],  test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 12\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i],  test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте нанесемо кілька зображень з їх передбаченнями. Зауважемо, що модель може помилятися, навіть якщо вона дуже впевнена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Побудуємо перші X тестових зображень, їхні передбачені мітки та справжні мітки.\n",
    "# Правильні передбачення пофарбуємо синім кольором, а неправильні – червоним.\n",
    "print(class_names)\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, predictions[i], test_labels, test_images)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарешті, використємо навчену модель, щоб спрогнозувати одне зображення."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Візьмемо зображення з тестового набору даних.\n",
    "img = test_images[1]\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Додаймо зображення до групи, де воно є єдиним членом.\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_single = probability_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_array(1, predictions_single[0], test_labels)\n",
    "_ = plt.xticks(range(10), class_names, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронні мережі в SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "random_state=42)\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "plot_decision_regions(X, y, clf=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10]).fit(X_train, y_train)\n",
    "plot_decision_regions(X, y, clf=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10, 10]).fit(X_train, y_train)\n",
    "plot_decision_regions(X, y, clf=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    solver='lbfgs', activation='tanh', random_state=0, hidden_layer_sizes=[10, 10]\n",
    ").fit(X_train, y_train)\n",
    "plot_decision_regions(X, y, clf=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "\n",
    "for axx, n_hidden_nodes in zip(axes, [10, 100]):\n",
    "    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n",
    "        mlp = MLPClassifier(\n",
    "            solver='lbfgs', \n",
    "            random_state=0,\n",
    "            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes], \n",
    "            alpha=alpha\n",
    "        )\n",
    "        mlp.fit(X_train, y_train)\n",
    "        plot_decision_regions(X, y, clf=mlp, ax=ax)\n",
    "        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(n_hidden_nodes, n_hidden_nodes, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
